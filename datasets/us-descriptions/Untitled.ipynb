{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c26282-6f2e-4694-ab9f-77c68bb1c4e3",
   "metadata": {},
   "source": [
    "# Sentence Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac211d5-2bed-456e-b9b9-7f6f1d05a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd6406b-3a61-4c80-ac89-5d6d0b01f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae8015-7bb6-4e89-97c5-0caa1d23ad2e",
   "metadata": {},
   "source": [
    "## Load spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63293e05-2a4e-42ab-afce-29b3c459630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m497.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce281de-cebe-44fe-9747-5dbcec6b13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a1f1f-5587-41eb-8337-1ab6dd5fe141",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf128ba-d69d-4500-a36d-51441004eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"mhurhangee/us-patent-descriptions\")\n",
    "train = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9152dc6a-a6c9-4b18-a580-bacf0ff6d416",
   "metadata": {},
   "source": [
    "## Helper: normalize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27d4d38-d439-4bf7-ac61-a7b49297f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_re = re.compile(r\"[^\\w\\s]\")\n",
    "space_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize(sent):\n",
    "    sent = sent.lower().strip()\n",
    "    sent = space_re.sub(\" \", sent)\n",
    "    sent = punct_re.sub(\"\", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6b8eb-536c-4c13-a0f0-3016157d9459",
   "metadata": {},
   "source": [
    "## Collect all sentences in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "492449f8-c5e3-4ece-815a-8690e04841f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15a317520304100bb8f1c8129be7d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m all_sents = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(train, total=\u001b[38;5;28mlen\u001b[39m(train)):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdescription_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc.sents:\n\u001b[32m      7\u001b[39m         norm = normalize(sent.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/spacy/language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/spacy/pipeline/tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/thinc/layers/maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_sents = []\n",
    "for row in tqdm(train, total=len(train)):\n",
    "    doc = nlp(row[\"description_text\"])\n",
    "    for sent in doc.sents:\n",
    "        norm = normalize(sent.text)\n",
    "        if len(norm.split()) > 3:\n",
    "            all_sents.append(norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd720-e4e4-4851-9ee8-230ffdcf84c4",
   "metadata": {},
   "source": [
    "V. slow 3 hours or more to analyse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687a480-449f-43f1-a3ca-144f8b9ecb4d",
   "metadata": {},
   "source": [
    "#  Rewriting to improve speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "166ab7ec-e777-44a5-94f1-956dad047b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7b724cb70310>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa40fcf0-d6be-4445-b9b1-c5da0c21fa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8594b276ca4360a60d6d5dbbcc5fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "/opt/conda/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "texts = [row[\"description_text\"] for row in train]\n",
    "\n",
    "for doc in tqdm(nlp.pipe(texts, batch_size=8, n_process=8), total=len(texts)):\n",
    "    for sent in doc.sents:\n",
    "        norm = normalize(sent.text)\n",
    "        if len(norm.split()) > 3:  # ignore very short fragments\n",
    "            all_sents.append(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ba6730-b798-4814-b396-e742fa14432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba2197ce-c6e2-40e7-8a52-5f9cb605f338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 : details are not described herein again\n",
      "246 : as used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise\n",
      "189 : as used herein the term andor includes any and all combinations of one or more of the associated listed items\n",
      "181 : these are of course merely examples and are not intended to be limiting\n",
      "168 : this repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments andor configurations discussed\n",
      "158 : the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures\n",
      "157 : in addition the present disclosure may repeat reference numerals andor letters in the various examples\n",
      "149 : the apparatus may be otherwise oriented rotated 90 degrees or at other orientations and the spatially relative descriptors used herein may likewise be interpreted accordingly\n",
      "144 : for example the formation of a first feature over or on a second feature in the description that follows may include embodiments in which the first and second features are formed in direct contact and may also include embodiments in which additional features may be formed between the first and second features such that the first and second features may not be in direct contact\n",
      "142 : specific examples of components and arrangements are described below to simplify the present disclosure\n"
     ]
    }
   ],
   "source": [
    "for sent, freq in counter.most_common(10):\n",
    "    print(freq, \":\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833e3cc-d4a8-42d0-b2ce-3e74f4dc5d94",
   "metadata": {},
   "source": [
    "Good but normalisation too aggressive. Instead, lets try with paragraphs -> sentences -> n-grams and keep normalisation minimal or map it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91b8285d-ebe2-484f-a085-623d66fbaf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DETAILED DESCRIPTION\\n\\nOverview\\n\\nThere can be a tradeoff between running workloads on cloud computing resources compared to edge computing resources, such as available processing power and latency. In some examples where a required response time is not very short or an amount of calculation required is extensively high, it can be clear that a given type of calculation should be performed on the cloud instead of the edge. However, in some examples, there can be a preference to running workloads on the edge to reduce networking bottlenecks and to distribute computation usage across edge nodes.\\n\\nWith respect to image processing systems, deciding whether to perform certain processing on the edge or on the cloud can vary according to specifics of a given image (e.g., a number of details in the image, or a number of areas of interest in the image). In some examples, a relatively high complexity of processing associated with an image can indicate that cloud processing is preferable to edge processing. In other examples, a relatively simple image to process can indicate that edge processing is preferable to cloud processing.\\n\\nThe present techniques can be implemented to improve runtime analysis of where to process an image by quickly and efficiently assessing a complexity of the image (e.g., by using artificial intelligence estimation techniques), and based on that assessment, determining where to process the image. A benefit of implementing the present techniques can be to provide a holistic solution that efficiently utilizes available computing resources.\\n\\nIn some examples, an estimation of the time it takes (e.g., computation complexity) for complicated inference tasks can be difficult to obtain. For example, when there are many faces in the image, conducting face recognition for that image can take significantly more time than compared to an image with a few faces. Without a proper estimation of the complexity of an image, there can little room for planning and scheduling image processing tasks.\\n\\nMaking a decision on an optimal, or chosen, configuration to accomplish a given calculation can be difficult, because an amount of time it takes to run the same abstract task can vary according to the input complexity. The input complexity can be different from the size of the image input. That is, two images with a same size can have different attributes and visual details that affect a complexity of processing the respective images.\\n\\nAt times, to meet service level agreement (SLA) requirements, it can be more efficient to communicate the image to a cloud instance, process the image, and return results to the edge, or pass over the subsequent systems.\\n\\nIn some examples, workloads can execute on the edge. Randomly, and as part of training, workloads can be replicated to a cloud instance as well, and an inference output can be made, as well as monitored metrics can be tracked (in some examples, the inference output produced by the cloud in training can be ignored). Following training, an estimation model can make decisions of where to perform image inference processing based on the image and the provided constraints. These techniques can be further optimized by running a test data set on the cloud and on the edge, gathering the respective metrics (e.g., processing time and network latency), and training the estimation model based on these gathered metrics.\\n\\nThe present techniques can be implemented to provide an estimation of image inference complexity. This estimation can be used for both planning and scheduling purposes.\\n\\nThe present techniques can also be implemented to balance image inference on the cloud and on the edge. That is, a decision of whether to perform an inference task on the cloud or on the edge can be made based on a measured complexity of the image. The present techniques can also be implemented to automate balancing cloud and edge. That is, a decision for a tradeoff between cloud and edge can be automatically made per image, for inference and recognition tasks.\\n\\nThe present techniques can be implemented to automatically balance inference for image processing systems deployed using a cloud-edge architecture. These techniques can include a learning component that makes a decision on routing to the cloud or keeping the task on the edge in order to meet performance constraints, obtain a service level, and optimize available resources.\\n\\nIn some examples, a cloud platform can train a model over many historical cases (where historical cases can be runs performed on an edge device, as well as on a cloud platform), and the model can be frequently updated offline (e.g., the cloud platform can update the model that is used by an edge device when the cloud platform is not currently communicating with the edge device about updating the model). A cloud platform can send a pre-trained model (along with its parameters) to an edge device, and the edge device can utilize this pre-trained model to run an inference (e.g., estimate) of processing speed according to edge parameters and a given workload (e.g., the model can process the parameters).\\n\\nExample Architectures\\n\\nFIG.1illustrates an example system architecture100that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. System architecture comprises edge device102a, edge device102b, edge device102c, cloud platform104, and communications network106.\\n\\nEdge device102acomprises model component108a, processing resources110a, data input component112a, and model training component114a. Edge device102bcomprises model component108b, processing resources110b, data input component112b, and model training component114b. Edge device102ccomprises model component108c, processing resources110c, data input component112c, and model training component114c. Cloud platform104comprises processing resources110dand model updating component116.\\n\\nEach of edge device102a, edge device102b, edge device102c, and cloud platform104can be implemented with part(s) of computing environment1000ofFIG.10.\\n\\nEach of edge device102a, edge device102b, edge device102ccan comprise an edge computing device. An edge computing device can be so named because it is located at the edge of a network, where the device gathers data with sensors (such as images or video). Cloud platform104can provide access to computing resources for each of edge device102a, edge device102b, edge device102c. For instance, edge device102acan capture an image and then transfer the image to cloud platform104via communications network106to storage or processing. In some examples, cloud platform104can provide more computing resources (e.g., faster processing, or more storage) than edge device102a, and those resources can be consumed, so that an amount of available resources can be lower than that of edge device102a.\\n\\nCommunications network106can comprise a computer communications network, such as the INTERNET.\\n\\nEach of model component108a, model component108b, and model component108ccan comprise a trained neural network model that takes an input (such as a video) and determines whether to perform a function on that input using the respective edge device upon which the model component operates, or on cloud platform104.\\n\\nFor instance, a model component can determine where to process an input based on whether it is faster to process it locally (on an edge device with relatively fewer processing resources, so the act of processing can be slower compared to a cloud platform) or in the cloud (where there can be relatively more processing resources, so the act of processing can be faster compared to on an edge device; though, there is a latency associated with transferring the image across a network, which can make the cloud an overall slower option).\\n\\nIn some examples, a neural network component can also consider constraints on the processing, such as a service level agreement, a cost of computation on the cloud, an amount of network congestion, an amount of cloud congestion, and a time of day.\\n\\nIn some examples, each of model component108a, model component108b, and model component108ccan be different (e.g., each model can have different weights) to reflect the circumstances of each respective edge device (e.g., some edge devices can have more processing resources relative to others, a faster connection to cloud platform relative to others, or different constraints relative to others).\\n\\nEach of processing resources110a, processing resources110b, processing resources110c, and processing resources110dcan comprise computer processing resources (such as provided by a microprocessor) that can be utilized to perform processing on input data, such as an image. Processing resources110a, processing resources110b, processing resources110c, and processing resources110dcan be different. For example, edge device102acan be more powerful than edge device102b. Or cloud platform104can have special hardware for performing a particular processing task that makes it faster to perform that task relative to an edge device.\\n\\nEach of data input component112a, data input component112b, and data input component112ccan comprise a sensor to generate input data, such as a camera to capture images or video, or a microphone to capture audio.\\n\\nModel training component114a, model training component114b, and model training component114ccan each train and update model component108a, model component108b, and model component108c, respectively. A model training component can use metrics gathered on the edge device (e.g., how much time it takes to process a particular input on the edge device), as well as metrics for the cloud received from model updating component116(e.g., how much time it takes to process a particular input on cloud platform104) and use this information to train a respective model component in determining where to process a particular input data-either on the edge device itself or on cloud platform104.\\n\\nModel updating component116can gather metrics relating to processing data on cloud platform104(e.g., how long it takes to process particular data), and share this with one or more model training components, which can use this data to update their respective model component.\\n\\nAs described with respect to system architecture100, there can generally be two stages to processing an input. One stage can be performed at an edge device, where the input is analyzed to determine where to perform additional processing-either on the edge device itself or on cloud platform104. Once that determination is made, a second stage of processing can occur at the determined location (cloud or edge). This second stage can comprise, e.g., performing facial recognition on an image or determining whether an intruder is detected in an image from a security system.\\n\\nIn the course of facilitating edge and cloud computing image processing, system architecture and/or edge device102a, edge device102b, and/or edge device102ccan implement part(s) of process flow500ofFIG.5, process flow600ofFIG.6, process flow700ofFIG.7, process flow800ofFIG.8, and/or process flow900ofFIG.9.\\n\\nFIG.2illustrates another example system architecture200that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. System architecture200comprises source data202, preprocessing component204, training edge and cloud computing image processing model component206, deploying edge and cloud computing image processing model component208, and output210.\\n\\nIn some examples, part(s) of system architecture200can be used to implement part(s) of edge device102a, edge device102, and/or edge device102cofFIG.1. In some examples, system architecture200can be implemented with part(s) of computing environment1000ofFIG.10.\\n\\nSource data202can comprise data that is analyzed to determine where to process the image (e.g., on edge or cloud), and then processed (e.g., to identify whether an intruder is present in an image for a security system). Source data202can be a dataset used for training an edge and cloud computing image processing model.\\n\\nPreprocessing component204can perform preprocessing tasks, such as cleaning, clipping, labeling, and normalizing source data202. Training edge and cloud computing image processing model component206can use this preprocessed data from preprocessing component204, and use it to train an edge and cloud computing image processing model to take input data (and in some examples, associated constraints) and produce an output that indicates whether to perform further processing on the input data locally or in the cloud.\\n\\nDeploying edge and cloud computing image processing model component208can comprise a deployed version of the model trained by training edge and cloud computing image processing model component206. Deploying edge and cloud computing image processing model component208can receive an input data and produce an output that indicates whether to perform further processing on the input data locally or in the cloud. This output can be output210.\\n\\nIn some examples, machine learning and AI-based image processing systems can comprise two main stages. One stage can be model training (e.g., training edge and cloud computing image processing model component206), where parameters of a model (such as a machine learning model or an AI model) can be optimized for a specific data set or use case. Another stage can be model inference, where a trained (e.g., optimized) model (e.g., in deploying edge and cloud computing image processing model component208) can be used to produce an outcome for a given image (e.g., where that outcome is an identification of whether to further process the image on edge or on cloud).\\n\\nTake an example security system, where an objective of the security system is to identify faces in a given image and associate those faces with a database of known individuals. Developing such a system can involve developing an AI system that identifies the presence of faces in an image.\\n\\nThen, a subsequent task can be to run these faces through a face recognition system, which can be a more computationally-expensive task as it can involve extracting features, comparing those extracted features to a database of known individuals, and returning information of whether a known individual is identified in the image.\\n\\nFIG.3illustrates another example system architecture300that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. System architecture300comprises input image302, preprocessing component304, face detection component306, face normalization component308, feature extraction component310, comparator component312, result314, and face and feature database316.\\n\\nIn some examples, part(s) of system architecture300can be used to implement part(s) of edge device102a, edge device102, edge device102c, and/or cloud platform104ofFIG.1. In some examples, system architecture300can be implemented with part(s) of computing environment1000ofFIG.10.\\n\\nInput image302can be a computer image for which processing is to be performed. Preprocessing component304can be similar to preprocessing component204ofFIG.2. Face detection component306can identify one or more faces within the image after it has been preprocessed with preprocessing component304. Face normalization component308can normalize an image of a face detected by face detection component306. Face normalization can comprise adjusting images so that they are similar—for example, so each face within an image is approximately the same size.\\n\\nFeature extraction component310can extract features from the image produced from face normalization component308. The output of both face normalization component308and feature extraction component310can be sent to face and feature database316, where both can be stored along with an association between the two types of data.\\n\\nComparator component can use the output of feature extraction component310and information in face and feature database316to determine if a face in input image302is identifiable based on information in face and feature database316. A result of this determination can be produced as result314.\\n\\nIn some examples, model development can be performed offline, using powerful computation systems over a period of time that is dedicated for a development process, and can involve continuous research and development using standard data sets collected and used throughout a development process. In some examples, data can be adjusted and expanded, and the computation environments used to run the model can vary, while overall this model development can be performed without considering constraints enforced during the deployment stage (while in some examples, development can be specifically done to address the constraints introduced by a production environment). That is, a model can be developed without consideration as to whether it will be deployed on the cloud (with relatively higher computing capabilities) or on the edge (with relatively lower computing capabilities).\\n\\nIn some examples, computing resources used to develop a model can resemble a powerful cloud infrastructure, and can include dedicated deep learning and AI processing units that can be deployed as part of a cloud instance.\\n\\nDeployment of face detection and recognition systems can be an evolved process. Reasons for this can include that video cameras are deployed in different remote locations; that response time can be extremely important, especially for security systems; and that an amount of data generated can be massive and lead to a bottleneck on a central compute system and on a network that at times might be clogged and/or remote.\\n\\nFor these and other reasons, such as an increase of computation power on Internet of Things (IoT) and edge compute devices, deployments of such systems can utilize a hub-and-spoke, or cloud-and-edge architecture.\\n\\nFIG.4illustrates another example system architecture400that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. System architecture400comprises edge428(which can be similar to edge device102aofFIG.1) and cloud (which can be similar to cloud platform104ofFIG.1). In turn, edge428comprises pre-inference432(which comprises images402, performance constraints404, and cloud/edge estimation neural network component406), cloud/edge?408, preprocessing component410, first step inference412, second step inference414, output416, and decision418. Cloud430comprises preprocessing component420, first step inference422, second step inference424, and output426.\\n\\nIn some examples, part(s) of system architecture300can be used to implement part(s) of system architecture100ofFIG.1. In some examples, system architecture400can be implemented with part(s) of computing environment1000ofFIG.10.\\n\\nImages402can comprise images captured by edge428that are to be processed, where edge428will determine whether to process the images on edge428or cloud430. Performance constraints404can comprise constraints on how images402are to be processed (e.g., a corresponding SLA). Cloud/edge estimation neural network component406can determine, based on images402, performance constraints404, metrics received from cloud430on how cloud430processes images, and metrics received from output416on how edge428processes images, whether to process each of images402locally or on cloud430.\\n\\nCloud/edge?408can route a particular image to cloud430or for local processing based on a determination of cloud/edge estimation neural network component406. Preprocessing component410, first step inference412, and second step inference414can comprise stages of processing an image on edge428, and a result of this processing can be output416.\\n\\nSimilarly, preprocessing component420, first step inference422, and second step inference424can comprise stages of processing an image on cloud430, and a result of this processing can be output426. Decision418can comprise a decision of processing an image of images402, such as whether an intruder is detected in the image from a security system, and can be made based on output426of cloud430or output416of edge428, depending on where the image was processed.\\n\\nThe present techniques can be implemented to apply a smaller neural network (relative to prior approaches) that, over time, learns to estimate the amount of processing time required for applying inference on each of the cloud and the edge given the image as input. This network can use underlying principles of a deep neural network for identifying relevant features that contribute to increased levels of complexity and calculation required for inference (which can use transfer learning and/or auto-encoding networks).\\n\\nOnce a model according to the present techniques is trained and designed, generating an estimate from the model can require few resources, and the model can generally improve in accuracy over time. Using these estimations of a complexity of an image, together with an optional required service level and response time, can facilitate optimizing an image processing system's performance, and meet target metrics.\\n\\nAn estimation network can comprise a small, efficient network that decides whether to conduct an image inference on the edge or on the cloud, given a specific input image and a set of constraints. These constraints can include a SLA, a cost of computation on the edge and in the cloud, an amount of network congestion, an amount of cloud congestion, and an hour of the day.\\n\\nIn some deep learning machine learning examples, feature engineering can be omitted as a model can learn features on its own during a training process. In these examples, a model's role can be to identify types of patterns in an image that can impact inference duration and computational complexity (e.g., quickly identify how many faces are present in an image for a face recognition system, or identifying a number of different pixels in an intrusion detection system). The model can then incorporate constraints given and produce a recommendation to run a workload on the cloud or keep it running on the edge.\\n\\nExample Process Flows\\n\\nFIG.5illustrates an example process flow500that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. In some examples, one or more embodiments of process flow500can be implemented by edge device102aofFIG.1, or computing environment1000ofFIG.10.\\n\\nIt can be appreciated that the operating procedures of process flow500are example operating procedures, and that there can be embodiments that implement more or fewer operating procedures than are depicted, or that implement the depicted operating procedures in a different order than as depicted. In some examples, process flow500can be implemented in conjunction with one or more embodiments of one or more of process flow600ofFIG.6, process flow700ofFIG.7, process flow800ofFIG.8, and/or process flow900ofFIG.9.\\n\\nProcess flow500begins with502, and moves to operation504.\\n\\nOperation504depicts training a neural network model at a first edge device regarding respective amounts of time to process data at the first edge device compared to corresponding amounts of time to process the data at cloud computing equipment that is connected to the first edge device via a communications network, wherein the data is generated at the first edge device. That is, a neural network on an edge device (e.g., edge device102aofFIG.1) can be trained (similar to training edge and cloud computing image processing model component206ofFIG.2) to determine whether to process input data locally or on a cloud platform (e.g., cloud platform104ofFIG.1). A trained neural network model can be similar to cloud/edge estimation neural network component406ofFIG.4. After operation504, process flow500moves to operation506.\\n\\nOperation506depicts updating the neural network model to produce an updated neural network model based on information received from a second edge device regarding a performance of the cloud computing equipment in processing the data, wherein the first edge device and the second edge device having respective different processing capabilities, and wherein the neural network model comprises a first neural network model that is trained with a first set of data, wherein the second edge device comprises a second neural network model that is trained with a second set of data, and wherein the first neural network model and the second neural network model differ.\\n\\nThat is, an edge device can update its model (regarding cloud platform performance) based on information received from another edge device. For example, using system architecture100ofFIG.1, edge device102acan update model component108abased on information about processing by cloud platform104received from edge device102bor edge device102c.\\n\\nIn some examples, first edge device is updated independently of updating a second edge device. That is, each edge device can have its own neural network model that is updating independently based on factors such as the respective edge device's processing capabilities, and a respective speed at which that edge device can transmit data to a cloud platform for processing.\\n\\nIn some examples, a first transmission time is associated with the first edge device transmitting the data to the cloud computing equipment for processing, a second transmission time is associated with the second edge device transmitting data to the cloud computing equipment for processing, and updating the neural network model is based on the first transmission time and the second transmission time. That is, there can be different transmission latencies between different edge devices and one cloud platform, and this information can be used when edge devices share information to update their respective models.\\n\\nAfter operation506, process flow500moves to operation508.\\n\\nOperation508depicts determining whether to process first data, generated at the first edge device, locally at the first edge device. That is, using system architecture100ofFIG.1, edge device102acan use an updated version of model component108ato determine whether input data from data input component112is to be processed locally at edge device102ausing processing resources110a, or is to be sent to cloud platform104for processing with processing resources110d.\\n\\nIn some examples, determining whether to process the first data locally at the first edge device comprises determining not to process the first data locally and determining to transmit the first data to the cloud computing equipment for processing based on the updated neural network model. That is, this determination can be to do one of processing locally and processing in the cloud, but not to both process locally and process in the cloud.\\n\\nIn some examples, a constraint is associated with the first data, and determining whether to process the first data locally is performed based on the constraint. That is, a constraint (such as a SLA) can be used to determine where to process the data—to process the data in a manner that adheres to the constraint.\\n\\nAfter operation508, process flow500moves to510, where process flow500ends.\\n\\nFIG.6illustrates another example process flow600that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. In some examples, one or more embodiments of process flow600can be implemented by edge device102aofFIG.1, or computing environment1000ofFIG.10.\\n\\nIt can be appreciated that the operating procedures of process flow600are example operating procedures, and that there can be embodiments that implement more or fewer operating procedures than are depicted, or that implement the depicted operating procedures in a different order than as depicted. In some examples, process flow600can be implemented in conjunction with one or more embodiments of one or more of process flow500ofFIG.5, process flow700ofFIG.7, process flow800ofFIG.8, and/or process flow900ofFIG.9.\\n\\nProcess flow600begins with602, and moves to operation604. Operation604depicts training a neural network model at a first edge device regarding respective amounts of time to process data locally compared to at a cloud computing platform, wherein the data is generated at the first edge device. In some examples, operation604can be implemented in a similar manner as operation504ofFIG.5.\\n\\nIn some examples, the neural network model is trained based on a service level agreement that indicates an acceptable latency associated with data processing. In some examples, the neural network model is trained based on a first monetary cost associated with the processing of the data being locally, and a second monetary cost associated with transmitting the data to the cloud computing platform for the processing. In some examples, the neural network model is trained based on an amount of network congestion associated with transmitting the data to the cloud computing platform. In some examples, the neural network model is trained based on an amount of available processing resources of the cloud computing platform. That is, constraints can be used to determine where to process data. These constraints can include a SLA, a cost of processing locally and a cost of processing in the cloud, network congestion to the cloud, and the cloud's load.\\n\\nIn some examples, the first edge device is associated with a first speed of processing data, the second edge device is associated with a second speed of processing data, and the first speed of processing data differs from the second speed of processing data. That is, edge devices can have different processing capabilities, and this difference can affect how\\n\\nAfter operation604, process flow600moves to operation606.\\n\\nOperation606depicts updating the neural network model resulting in an updated neural network model based on information received from a second edge device regarding a data processing performance of the cloud computing platform. In some examples, operation606can be implemented in a similar manner as operation606ofFIG.5.\\n\\nAfter operation606, process flow600moves to operation608.\\n\\nOperation608depicts, in response to first data being generated at the first edge device, determining, using the updated neural network model, whether to transmit first data to the cloud computing platform for processing. In some examples, operation608can be implemented in a similar manner as operation508ofFIG.5.\\n\\nIn some examples, operation608comprises determining not to transmit the first data to the cloud computing platform for the processing and determining to process the first data locally at the first edge device. That is, the determination can be made to do one of processing particular data in the cloud or doing that processing locally.\\n\\nAfter operation608, process flow600moves to610, where process flow600ends.\\n\\nFIG.7illustrates another example process flow700that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. In some examples, one or more embodiments of process flow700can be implemented by edge device102aofFIG.1, or computing environment1000ofFIG.10.\\n\\nIt can be appreciated that the operating procedures of process flow700are example operating procedures, and that there can be embodiments that implement more or fewer operating procedures than are depicted, or that implement the depicted operating procedures in a different order than as depicted. In some examples, process flow700can be implemented in conjunction with one or more embodiments of one or more of process flow500ofFIG.5, process flow600ofFIG.6, process flow800ofFIG.8, and/or process flow900ofFIG.9.\\n\\nProcess flow700begins with702, and moves to operation704. Operation704depicts training a neural network model at an edge device regarding respective amounts of time to process data locally compared to amounts of time to process the data at network equipment remote from the edge device, wherein the data is generated at the edge device. In some examples, operation704can be implemented in a similar manner as operations504and506ofFIG.5.\\n\\nIn some examples, the edge device is a first edge device, and training the neural network model is performed based on information received from a second edge device regarding a performance of the network equipment in processing the data from the second edge device. That is, multiple edge nodes can share information about cloud performance in processing data, and respective edge devices can use this information to train and/or update their respective model.\\n\\nIn some examples, the neural network model is trained based on a time of day at which the data is processed. That is, a time of day at which processing will occur can be a constraint that is used to determine where to do the processing.\\n\\nAfter operation704, process flow700moves to operation706.\\n\\nOperation706depicts determining whether to process first data, generated at the edge device, locally or to transmit the first data to the network equipment for processing. In some examples, operation706can be implemented in a similar manner as operation508ofFIG.5.\\n\\nIn some examples, determining whether to process the first data locally or to transmit the first data to the network equipment for processing is performed based on a content of the first data. That is, a routing decision can be made based on contents of data that is to be processed (e.g., a complexity of an image, such as a number of faces to recognize).\\n\\nAfter operation706, process flow700moves to708, where process flow700ends.\\n\\nFIG.8illustrates another example process flow800for subdividing processing between edge and cloud computing that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. In some examples, one or more embodiments of process flow800can be implemented by edge device102aofFIG.1, or computing environment1000ofFIG.10.\\n\\nIt can be appreciated that the operating procedures of process flow800are example operating procedures, and that there can be embodiments that implement more or fewer operating procedures than are depicted, or that implement the depicted operating procedures in a different order than as depicted. In some examples, process flow800can be implemented in conjunction with one or more embodiments of one or more of process flow500ofFIG.5, process flow600ofFIG.6, process flow700ofFIG.7, and/or process flow900ofFIG.9.\\n\\nProcess flow800begins with802, and moves to operation804. In some examples, processing data comprises performing a first operation and a second operation, and determining whether to process data locally at an edge device or in the cloud can be performed as follows. Operation804depicts performing the first operation on the first edge device independent of the determining whether to process the first data locally. This can comprise pre-processing of the data that, in some examples, is always performed locally. In a facial recognition example, this pre-processing can comprise locating faces in the image.\\n\\nIn some examples, operation804can comprise performing a first operation of the processing on the edge device independent of the determining of whether to process the first data locally or to transmit the first data to network equipment for the processing. After operation804, process flow800moves to operation806.\\n\\nOperation806depicts determining whether to perform the second operation locally or to transmit the first data to the cloud computing equipment for processing. This can comprise, after pre-processing the data, determining whether to perform additional processing at an edge device or in the cloud.\\n\\nUsing the facial recognition example of operation804, the first operation can comprise pre-processing by locating faces in an image, and then the second operation can comprise performing facial recognition on these faces that were located in operation804. That is, in some examples, the first operation comprises identifying, within an image, a presence of facial data indicative of a face, and the second operation comprises performing a facial recognition on the facial data to identify the face.\\n\\nIn some examples, operation806can comprise determining whether to perform a second operation of the processing locally or to transmit the first data to the network equipment for the processing.\\n\\nAfter operation806, process flow800moves to808, where process flow800ends.\\n\\nFIG.9illustrates another example process flow900for training a model that can facilitate edge and cloud computing image processing, in accordance with an embodiment of this disclosure. In some examples, one or more embodiments of process flow900can be implemented by edge device102aofFIG.1, or computing environment1000ofFIG.10.\\n\\nIt can be appreciated that the operating procedures of process flow900are example operating procedures, and that there can be embodiments that implement more or fewer operating procedures than are depicted, or that implement the depicted operating procedures in a different order than as depicted. In some examples, process flow900can be implemented in conjunction with one or more embodiments of one or more of process flow500ofFIG.5, process flow600ofFIG.6, process flow700ofFIG.7, and/or process flow800ofFIG.8.\\n\\nProcess flow900begins with902, and moves to operation904. In some examples, training a model can comprise executing workloads on an edge by default, and randomly replicating workloads to a cloud (so they are processed in both locations) to produce monitored metrics relating to the processing that can indicate a speed of processing on edge relative to a speed of processing in the cloud. Operation904depicts processing a first workload with the edge device to produce first metrics. That is, a workload (e.g., images) can be processed on an edge device that captures that workload.\\n\\nAfter operation904, process flow900moves to operation906.\\n\\nOperation906depicts replicating the first workload from the edge device to the network equipment for processing by the network equipment to produce second metrics. That is, that workload can be copied to a cloud platform for processing in addition to being processed locally in operation904.\\n\\nAfter operation906, process flow900moves to operation908.\\n\\nOperation908depicts training the neural network model based on the first metrics and the second metrics. That is, information on both processing the workload locally and processing the workload in the cloud can be used to train the local model that determines where to process future workloads.\\n\\nAfter operation908, process flow900moves to910, where process flow900ends.\\n\\nExample Operating Environment\\n\\nIn order to provide additional context for various embodiments described herein,FIG.10and the following discussion are intended to provide a brief, general description of a suitable computing environment1000in which the various embodiments of the embodiment described herein can be implemented.\\n\\nFor example, parts of computing environment1000can be used to implement one or more embodiments of edge device102a, edge device102b, edge device102c, and/or cloud platform104ofFIG.1.\\n\\nIn some examples, computing environment1000can implement one or more embodiments of the process flows ofFIGS.5-9to facilitate edge and cloud computing image processing.\\n\\nWhile the embodiments have been described above in the general context of computer-executable instructions that can run on one or more computers, those skilled in the art will recognize that the embodiments can be also implemented in combination with other program modules and/or as a combination of hardware and software.\\n\\nGenerally, program modules include routines, programs, components, data structures, etc., that perform particular tasks or implement particular abstract data types. Moreover, those skilled in the art will appreciate that the various methods can be practiced with other computer system configurations, including single-processor or multiprocessor computer systems, minicomputers, mainframe computers, Internet of Things (IoT) devices, distributed computing systems, as well as personal computers, hand-held computing devices, microprocessor-based or programmable consumer electronics, and the like, each of which can be operatively coupled to one or more associated devices.\\n\\nThe illustrated embodiments of the embodiments herein can be also practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules can be located in both local and remote memory storage devices.\\n\\nComputing devices typically include a variety of media, which can include computer-readable storage media, machine-readable storage media, and/or communications media, which two terms are used herein differently from one another as follows. Computer-readable storage media or machine-readable storage media can be any available storage media that can be accessed by the computer and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer-readable storage media or machine-readable storage media can be implemented in connection with any method or technology for storage of information such as computer-readable or machine-readable instructions, program modules, structured data or unstructured data.\\n\\nComputer-readable storage media can include, but are not limited to, random access memory (RAM), read only memory (ROM), electrically erasable programmable read only memory (EEPROM), flash memory or other memory technology, compact disk read only memory (CD-ROM), digital versatile disk (DVD), Blu-ray disc (BD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, solid state drives or other solid state storage devices, or other tangible and/or non-transitory media which can be used to store desired information. In this regard, the terms “tangible” or “non-transitory” herein as applied to storage, memory or computer-readable media, are to be understood to exclude only propagating transitory signals per se as modifiers and do not relinquish rights to all standard storage, memory or computer-readable media that are not only propagating transitory signals per se.\\n\\nComputer-readable storage media can be accessed by one or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to the information stored by the medium.\\n\\nCommunications media typically embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other transport mechanism, and includes any information delivery or transport media. The term “modulated data signal” or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or more signals. By way of example, and not limitation, communication media include wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.\\n\\nWith reference again toFIG.10, the example environment1000for implementing various embodiments described herein includes a computer1002, the computer1002including a processing unit1004, a system memory1006and a system bus1008. The system bus1008couples system components including, but not limited to, the system memory1006to the processing unit1004. The processing unit1004can be any of various commercially available processors. Dual microprocessors and other multiprocessor architectures can also be employed as the processing unit1004.\\n\\nThe system bus1008can be any of several types of bus structure that can further interconnect to a memory bus (with or without a memory controller), a peripheral bus, and a local bus using any of a variety of commercially available bus architectures. The system memory1006includes ROM1010and RAM1012. A basic input/output system (BIOS) can be stored in a nonvolatile storage such as ROM, erasable programmable read only memory (EPROM), EEPROM, which BIOS contains the basic routines that help to transfer information between elements within the computer1002, such as during startup. The RAM1012can also include a high-speed RAM such as static RAM for caching data.\\n\\nThe computer1002further includes an internal hard disk drive (HDD)1014(e.g., EIDE, SATA), one or more external storage devices1016(e.g., a magnetic floppy disk drive (FDD)1016, a memory stick or flash drive reader, a memory card reader, etc.) and an optical disk drive1020(e.g., which can read or write from a CD-ROM disc, a DVD, a BD, etc.). While the internal HDD1014is illustrated as located within the computer1002, the internal HDD1014can also be configured for external use in a suitable chassis (not shown). Additionally, while not shown in environment1000, a solid state drive (SSD) could be used in addition to, or in place of, an HDD1014. The HDD1014, external storage device(s)1016and optical disk drive1020can be connected to the system bus1008by an HDD interface1024, an external storage interface1026and an optical drive interface1028, respectively. The interface1024for external drive implementations can include at least one or both of Universal Serial Bus (USB) and Institute of Electrical and Electronics Engineers (IEEE) 1394 interface technologies. Other external drive connection technologies are within contemplation of the embodiments described herein.\\n\\nThe drives and their associated computer-readable storage media provide nonvolatile storage of data, data structures, computer-executable instructions, and so forth. For the computer1002, the drives and storage media accommodate the storage of any data in a suitable digital format. Although the description of computer-readable storage media above refers to respective types of storage devices, it should be appreciated by those skilled in the art that other types of storage media which are readable by a computer, whether presently existing or developed in the future, could also be used in the example operating environment, and further, that any such storage media can contain computer-executable instructions for performing the methods described herein.\\n\\nA number of program modules can be stored in the drives and RAM1012, including an operating system1030, one or more application programs1032, other program modules1034and program data1036. All or portions of the operating system, applications, modules, and/or data can also be cached in the RAM1012. The systems and methods described herein can be implemented utilizing various commercially available operating systems or combinations of operating systems.\\n\\nComputer1002can optionally comprise emulation technologies. For example, a hypervisor (not shown) or other intermediary can emulate a hardware environment for operating system1030, and the emulated hardware can optionally be different from the hardware illustrated inFIG.10. In such an embodiment, operating system1030can comprise one virtual machine (VM) of multiple VMs hosted at computer1002. Furthermore, operating system1030can provide runtime environments, such as the Java runtime environment or the NET framework, for applications1032. Runtime environments are consistent execution environments that allow applications1032to run on any operating system that includes the runtime environment. Similarly, operating system1030can support containers, and applications1032can be in the form of containers, which are lightweight, standalone, executable packages of software that include, e.g., code, runtime, system tools, system libraries and settings for an application.\\n\\nFurther, computer1002can be enabled with a security module, such as a trusted processing module (TPM). For instance, with a TPM, boot components hash next in time boot components, and wait for a match of results to secured values, before loading a next boot component. This process can take place at any layer in the code execution stack of computer1002, e.g., applied at the application execution level or at the operating system (OS) kernel level, thereby enabling security at any level of code execution.\\n\\nA user can enter commands and information into the computer1002through one or more wired/wireless input devices, e.g., a keyboard1038, a touch screen1040, and a pointing device, such as a mouse1042. Other input devices (not shown) can include a microphone, an infrared (IR) remote control, a radio frequency (RF) remote control, or other remote control, a joystick, a virtual reality controller and/or virtual reality headset, a game pad, a stylus pen, an image input device, e.g., camera(s), a gesture sensor input device, a vision movement sensor input device, an emotion or facial detection device, a biometric input device, e.g., fingerprint or iris scanner, or the like. These and other input devices are often connected to the processing unit1004through an input device interface1044that can be coupled to the system bus1008, but can be connected by other interfaces, such as a parallel port, an IEEE 1394 serial port, a game port, a USB port, an IR interface, a BLUETOOTH® interface, etc.\\n\\nA monitor1046or other type of display device can be also connected to the system bus1008via an interface, such as a video adapter1048. In addition to the monitor1046, a computer typically includes other peripheral output devices (not shown), such as speakers, printers, etc.\\n\\nThe computer1002can operate in a networked environment using logical connections via wired and/or wireless communications to one or more remote computers, such as a remote computer(s)1050. The remote computer(s)1050can be a workstation, a server computer, a router, a personal computer, portable computer, microprocessor-based entertainment appliance, a peer device or other common network node, and typically includes many or all of the elements described relative to the computer1002, although, for purposes of brevity, only a memory/storage device1052is illustrated. The logical connections depicted include wired/wireless connectivity to a local area network (LAN)1054and/or larger networks, e.g., a wide area network (WAN)1056. Such LAN and WAN networking environments are commonplace in offices and companies, and facilitate enterprise-wide computer networks, such as intranets, all of which can connect to a global communications network, e.g., the Internet.\\n\\nWhen used in a LAN networking environment, the computer1002can be connected to the local network1054through a wired and/or wireless communication network interface or adapter1058. The adapter1058can facilitate wired or wireless communication to the LAN1054, which can also include a wireless access point (AP) disposed thereon for communicating with the adapter1058in a wireless mode.\\n\\nWhen used in a WAN networking environment, the computer1002can include a modem1060or can be connected to a communications server on the WAN1056via other means for establishing communications over the WAN1056, such as by way of the Internet. The modem1060, which can be internal or external and a wired or wireless device, can be connected to the system bus1008via the input device interface1044. In a networked environment, program modules depicted relative to the computer1002or portions thereof, can be stored in the remote memory/storage device1052. It will be appreciated that the network connections shown are examples, and other means of establishing a communications link between the computers can be used.\\n\\nWhen used in either a LAN or WAN networking environment, the computer1002can access cloud storage systems or other network-based storage systems in addition to, or in place of, external storage devices1016as described above. Generally, a connection between the computer1002and a cloud storage system can be established over a LAN1054or WAN1056e.g., by the adapter1058or modem1060, respectively. Upon connecting the computer1002to an associated cloud storage system, the external storage interface1026can, with the aid of the adapter1058and/or modem1060, manage storage provided by the cloud storage system as it would other types of external storage. For instance, the external storage interface1026can be configured to provide access to cloud storage sources as if those sources were physically connected to the computer1002.\\n\\nThe computer1002can be operable to communicate with any wireless devices or entities operatively disposed in wireless communication, e.g., a printer, scanner, desktop and/or portable computer, portable data assistant, communications satellite, any piece of equipment or location associated with a wirelessly detectable tag (e.g., a kiosk, news stand, store shelf, etc.), and telephone. This can include Wireless Fidelity (Wi-Fi) and BLUETOOTH® wireless technologies. Thus, the communication can be a predefined structure as with a conventional network or simply an ad hoc communication between at least two devices.\\n\\nCONCLUSION\\n\\nAs it employed in the subject specification, the term “processor” can refer to substantially any computing processing unit or device comprising, but not limited to comprising, single-core processors; single-processors with software multithread execution capability; multi-core processors; multi-core processors with software multithread execution capability; multi-core processors with hardware multithread technology; parallel platforms; and parallel platforms with distributed shared memory in a single machine or multiple machines. Additionally, a processor can refer to an integrated circuit, a state machine, an application specific integrated circuit (ASIC), a digital signal processor (DSP), a programmable gate array (PGA) including a field programmable gate array (FPGA), a programmable logic controller (PLC), a complex programmable logic device (CPLD), a discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. Processors can exploit nano-scale architectures such as, but not limited to, molecular and quantum-dot based transistors, switches and gates, in order to optimize space usage or enhance performance of user equipment. A processor may also be implemented as a combination of computing processing units. One or more processors can be utilized in supporting a virtualized computing environment. The virtualized computing environment may support one or more virtual machines representing computers, servers, or other computing devices. In such virtualized virtual machines, components such as processors and storage devices may be virtualized or logically represented. For instance, when a processor executes instructions to perform “operations”, this could include the processor performing the operations directly and/or facilitating, directing, or cooperating with another device or component to perform the operations.\\n\\nIn the subject specification, terms such as “data store,” data storage,” “database,” “cache,” and substantially any other information storage component relevant to operation and functionality of a component, refer to “memory components,” or entities embodied in a “memory” or components comprising the memory. It will be appreciated that the memory components, or computer-readable storage media, described herein can be either volatile memory or nonvolatile storage, or can include both volatile and nonvolatile storage. By way of illustration, and not limitation, nonvolatile storage can include ROM, programmable ROM (PROM), EPROM, EEPROM, or flash memory. Volatile memory can include RAM, which acts as external cache memory. By way of illustration and not limitation, RAM can be available in many forms such as synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR SDRAM), enhanced SDRAM (ESDRAM), Synchlink DRAM (SLDRAM), and direct Rambus RAM (DRRAM). Additionally, the disclosed memory components of systems or methods herein are intended to comprise, without being limited to comprising, these and any other suitable types of memory.\\n\\nThe illustrated embodiments of the disclosure can be practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules can be located in both local and remote memory storage devices.\\n\\nThe systems and processes described above can be embodied within hardware, such as a single integrated circuit (IC) chip, multiple ICs, an ASIC, or the like. Further, the order in which some or all of the process blocks appear in each process should not be deemed limiting. Rather, it should be understood that some of the process blocks can be executed in a variety of orders that are not all of which may be explicitly illustrated herein.\\n\\nAs used in this application, the terms “component,” “module,” “system,” “interface,” “cluster,” “server,” “node,” or the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution or an entity related to an operational machine with one or more specific functionalities. For example, a component can be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, computer-executable instruction(s), a program, and/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers. As another example, an interface can include input/output (I/O) components as well as associated processor, application, and/or application programming interface (API) components.\\n\\nFurther, the various embodiments can be implemented as a method, apparatus, or article of manufacture using standard programming and/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a computer to implement one or more embodiments of the disclosed subject matter. An article of manufacture can encompass a computer program accessible from any computer-readable device or computer-readable storage/communications media. For example, computer readable storage media can include but are not limited to magnetic storage devices (e.g., hard disk, floppy disk, magnetic strips . . . ), optical discs (e.g., CD, DVD . . . ), smart cards, and flash memory devices (e.g., card, stick, key drive . . . ). Of course, those skilled in the art will recognize many modifications can be made to this configuration without departing from the scope or spirit of the various embodiments.\\n\\nIn addition, the word “example” or “exemplary” is used herein to mean serving as an example, instance, or illustration. Any embodiment or design described herein as “exemplary” is not necessarily to be construed as preferred or advantageous over other embodiments or designs. Rather, use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise, or clear from context, “X employs A or B” is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then “X employs A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more” unless specified otherwise or clear from context to be directed to a singular form.\\n\\nWhat has been described above includes examples of the present specification. It is, of course, not possible to describe every conceivable combination of components or methods for purposes of describing the present specification, but one of ordinary skill in the art may recognize that many further combinations and permutations of the present specification are possible. Accordingly, the present specification is intended to embrace all such alterations, modifications and variations that fall within the spirit and scope of the appended claims. Furthermore, to the extent that the term “includes” is used in either the detailed description or the claims, such term is intended to be inclusive in a manner similar to the term “comprising” as “comprising” is interpreted when employed as a transitional word in a claim.\\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
